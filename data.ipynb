{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class IMBALANCECIFAR10(datasets.CIFAR10):\n",
    "    cls_num = 10\n",
    "\n",
    "    def __init__(self, phase, imbalance_ratio, root='data/cifar10_lt/', imb_type='exp', train_aug=True):\n",
    "        train = True if phase == 'train' else False\n",
    "        super(IMBALANCECIFAR10, self).__init__(root, train, transform=None, target_transform=None, download=True)\n",
    "        self.train = train\n",
    "        if self.train:\n",
    "            img_num_list = self.get_img_num_per_cls(self.cls_num, imb_type, imbalance_ratio)\n",
    "            self.gen_imbalanced_data(img_num_list)\n",
    "            if train_aug:\n",
    "                self.transform = transforms.Compose([\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "                    ])\n",
    "            else:\n",
    "                self.transform = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "                ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "                ])\n",
    "\n",
    "        self.labels = self.targets\n",
    "\n",
    "        print('{} Mode: Contain {} images'.format(phase, len(self.data)))\n",
    "\n",
    "    def _get_class_dict(self):\n",
    "        class_dict = dict()\n",
    "        for i, anno in enumerate(self.get_annotations()):\n",
    "            cat_id = anno[\"category_id\"]\n",
    "            if not cat_id in class_dict:\n",
    "                class_dict[cat_id] = []\n",
    "            class_dict[cat_id].append(i)\n",
    "        return class_dict\n",
    "\n",
    "    def get_img_num_per_cls(self, cls_num, imb_type, imb_factor):\n",
    "        img_max = len(self.data) / cls_num\n",
    "        img_num_per_cls = []\n",
    "        if imb_type == 'exp':\n",
    "            for cls_idx in range(cls_num):\n",
    "                num = img_max * (imb_factor**(cls_idx / (cls_num - 1.0)))\n",
    "                img_num_per_cls.append(int(num))\n",
    "        elif imb_type == 'step':\n",
    "            for cls_idx in range(cls_num // 2):\n",
    "                img_num_per_cls.append(int(img_max))\n",
    "            for cls_idx in range(cls_num // 2):\n",
    "                img_num_per_cls.append(int(img_max * imb_factor))\n",
    "        else:\n",
    "            img_num_per_cls.extend([int(img_max)] * cls_num)\n",
    "        return img_num_per_cls\n",
    "\n",
    "    def gen_imbalanced_data(self, img_num_per_cls):\n",
    "        new_data = []\n",
    "        new_targets = []\n",
    "        targets_np = np.array(self.targets, dtype=np.int64)\n",
    "        classes = np.unique(targets_np)\n",
    "\n",
    "        self.num_per_cls_dict = dict()\n",
    "        for the_class, the_img_num in zip(classes, img_num_per_cls):\n",
    "            self.num_per_cls_dict[the_class] = the_img_num\n",
    "            idx = np.where(targets_np == the_class)[0]\n",
    "            np.random.shuffle(idx)\n",
    "            selec_idx = idx[:the_img_num]\n",
    "            new_data.append(self.data[selec_idx, ...])\n",
    "            new_targets.extend([the_class, ] * the_img_num)\n",
    "        new_data = np.vstack(new_data)\n",
    "        self.data = new_data\n",
    "        self.targets = new_targets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, label = self.data[index], self.labels[index]\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return img, label #, index\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_num_classes(self):\n",
    "        return self.cls_num\n",
    "\n",
    "    def get_annotations(self):\n",
    "        annos = []\n",
    "        for label in self.labels:\n",
    "            annos.append({'category_id': int(label)})\n",
    "        return annos\n",
    "        \n",
    "    def get_cls_num_list(self):\n",
    "        cls_num_list = []\n",
    "        for i in range(self.cls_num):\n",
    "            cls_num_list.append(self.num_per_cls_dict[i])\n",
    "        return cls_num_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mrhao\\anaconda3\\envs\\RobustFed\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets, transforms\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_model\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfl_methods\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_fl_method_class\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mquery_strategies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m random_query_samples, algo_query_samples\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Python version: 3.9\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import copy\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import medmnist\n",
    "from medmnist import INFO\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from util.args import args_parser\n",
    "from util.path import set_result_dir, set_dict_user_path\n",
    "from util.data_simulator import shard_balance, dir_balance\n",
    "from util.longtail_dataset import IMBALANCECIFAR10, IMBALANCECIFAR100\n",
    "from util.misc import adjust_learning_rate\n",
    "\n",
    "\n",
    "def get_dataset(args):\n",
    "    MEAN = {'mnist': (0.1307,), 'fmnist': (0.5,), 'emnist': (0.5,), 'svhn': [0.4376821, 0.4437697, 0.47280442], \n",
    "            'cifar10': [0.485, 0.456, 0.406], 'cifar100': [0.507, 0.487, 0.441], 'pathmnist': (0.5,), \n",
    "            'octmnist': (0.5,), 'organamnist': (0.5,), 'dermamnist': (0.5,), 'bloodmnist': (0.5,)}\n",
    "    STD = {'mnist': (0.3081,), 'fmnist': (0.5,), 'emnist': (0.5,), 'svhn': [0.19803012, 0.20101562, 0.19703614], \n",
    "           'cifar10': [0.229, 0.224, 0.225], 'cifar100': [0.267, 0.256, 0.276], 'pathmnist': (0.5,),\n",
    "           'octmnist': (0.5,), 'organamnist': (0.5,), 'dermamnist': (0.5,), 'bloodmnist': (0.5,)}\n",
    "    \n",
    "    if 'lt' not in args.dataset:\n",
    "        noaug = [transforms.ToTensor(),\n",
    "                 transforms.Normalize(mean=MEAN[args.dataset], std=STD[args.dataset])]\n",
    "        \n",
    "        weakaug = [transforms.RandomHorizontalFlip(),\n",
    "                   transforms.ToTensor(),\n",
    "                   transforms.Normalize(mean=MEAN[args.dataset], std=STD[args.dataset])]\n",
    "        \n",
    "        trans_noaug = transforms.Compose(noaug)\n",
    "        trans_weakaug = transforms.Compose(weakaug)\n",
    "        \n",
    "    # standard benchmarks\n",
    "    print('Load Dataset {}'.format(args.dataset))\n",
    "    if args.dataset == 'mnist':\n",
    "        dataset_train = datasets.MNIST(args.data_dir, train=True, download=True, transform=trans_weakaug)\n",
    "        dataset_query = datasets.MNIST(args.data_dir, train=True, download=True, transform=trans_noaug)\n",
    "        dataset_test = datasets.MNIST(args.data_dir, train=False, download=True, transform=trans_noaug)\n",
    "    \n",
    "    elif args.dataset == \"fmnist\":\n",
    "        dataset_train = datasets.FashionMNIST(args.data_dir, download=True, train=True, transform=trans_weakaug)\n",
    "        dataset_query = datasets.FashionMNIST(args.data_dir, download=True, train=True, transform=trans_noaug)\n",
    "        dataset_test = datasets.FashionMNIST(args.data_dir, download=True, train=False, transform=trans_noaug)\n",
    "\n",
    "    elif args.dataset == 'emnist':\n",
    "        dataset_train = datasets.EMNIST(args.data_dir, split='byclass', train=True, download=True, transform=trans_weakaug)\n",
    "        dataset_query = datasets.EMNIST(args.data_dir, split='byclass', train=True, download=True, transform=trans_noaug)\n",
    "        dataset_test = datasets.EMNIST(args.data_dir, split='byclass', train=False, download=True, transform=trans_noaug)\n",
    "\n",
    "    elif args.dataset == 'svhn':\n",
    "        dataset_train = datasets.SVHN(args.data_dir, 'train', download=True, transform=trans_weakaug)\n",
    "        dataset_query = datasets.SVHN(args.data_dir, 'train', download=True, transform=trans_noaug)\n",
    "        dataset_test = datasets.SVHN(args.data_dir, 'test', download=True, transform=trans_noaug)\n",
    "            \n",
    "    elif args.dataset == 'cifar10':\n",
    "        dataset_train = datasets.CIFAR10(args.data_dir, train=True, download=True, transform=trans_weakaug)\n",
    "        dataset_query = datasets.CIFAR10(args.data_dir, train=True, download=True, transform=trans_noaug)\n",
    "        dataset_test = datasets.CIFAR10(args.data_dir, train=False, download=True, transform=trans_noaug)\n",
    "            \n",
    "    elif args.dataset == 'cifar10_lt':\n",
    "        dataset_train = IMBALANCECIFAR10('train', args.imb_ratio, args.data_dir)\n",
    "        dataset_query = IMBALANCECIFAR10('train', args.imb_ratio, args.data_dir, train_aug=False)\n",
    "        dataset_test = IMBALANCECIFAR10('test', args.imb_ratio, args.data_dir)\n",
    "        \n",
    "    elif args.dataset == 'cifar100':\n",
    "        dataset_train = datasets.CIFAR100(args.data_dir, train=True, download=True, transform=trans_weakaug)\n",
    "        dataset_query = datasets.CIFAR100(args.data_dir, train=True, download=True, transform=trans_noaug)\n",
    "        dataset_test = datasets.CIFAR100(args.data_dir, train=False, download=True, transform=trans_noaug)\n",
    "            \n",
    "    elif args.dataset == 'cifar10_lt':\n",
    "        dataset_train = IMBALANCECIFAR100('train', args.imb_ratio, args.data_dir)\n",
    "        dataset_query = IMBALANCECIFAR100('train', args.imb_ratio, args.data_dir, train_aug=False)\n",
    "        dataset_test = IMBALANCECIFAR100('test', args.imb_ratio, args.data_dir)\n",
    "\n",
    "    # medical benchmarks\n",
    "    elif args.dataset in ['pathmnist', 'octmnist', 'organamnist', 'dermamnist', 'bloodmnist']:\n",
    "        DataClass = getattr(medmnist, INFO[args.dataset]['python_class'])\n",
    "        \n",
    "        dataset_train = DataClass(download=True, split='train', transform=trans_weakaug)\n",
    "        dataset_query = DataClass(download=True, split='train', transform=trans_noaug)\n",
    "        dataset_test = DataClass(download=True, split='test', transform=trans_noaug)\n",
    "        \n",
    "    else:\n",
    "        exit('Error: unrecognized dataset')\n",
    "        \n",
    "    args.dataset_train = dataset_train\n",
    "    args.total_data = len(dataset_train)\n",
    "\n",
    "    if args.partition == \"shard_balance\":\n",
    "        dict_users_train_total = shard_balance(dataset_train, args)\n",
    "        dict_users_test_total = shard_balance(dataset_test, args)\n",
    "    elif args.partition == \"dir_balance\":\n",
    "        dict_users_train_total, sample = dir_balance(dataset_train, args)\n",
    "        dict_users_test_total, _ = dir_balance(dataset_test, args, sample)\n",
    "    \n",
    "    args.n_query = round(args.total_data, -2) * args.query_ratio\n",
    "    args.n_data = round(args.total_data, -2) * args.current_ratio\n",
    "    \n",
    "    return dataset_train, dataset_query, dataset_test, dict_users_train_total, dict_users_test_total, args"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RobustFed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
